---
# Source: spark/templates/statefulset-worker.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: spark-worker-rev-36
  namespace: "default"
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-7.1.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: worker
spec:
  serviceName: release-name-spark-headless
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-7.1.1
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: worker
    spec:
      
      serviceAccountName: release-name-spark
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: spark
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: worker
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        runAsGroup: 0
        runAsUser: 1001
        seLinuxOptions: {}
      containers:
        - name: spark-worker
          image: docker.io/bitnami/spark:3.4.1-debian-11-r6
          imagePullPolicy: "IfNotPresent"
          securityContext:
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          volumeMounts:
          env:
            - name: SPARK_MODE
              value: "worker"
            - name: BITNAMI_DEBUG
              value: "false"
            - name: SPARK_DAEMON_MEMORY
              value: ""
            ## There are some environment variables whose existence needs
            ## to be checked because Spark checks if they are null instead of an
            ## empty string
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8080"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: ""
            - name: SPARK_MASTER_URL
              value: spark://release-name-spark-master-svc:7077
            # If you use a custom properties file, it must be loaded using a ConfigMap
            - name: SPARK_WORKER_OPTS
              value: 
          envFrom:
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 180
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: 8080
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: 8080
          startupProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: 8080
          resources:
            limits: {}
            requests: {}
      volumes:
